{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 6: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. The windy gridworld domain\n",
    "\n",
    "Consider the larger version of the windy gridworld domain depicted in the figure below.\n",
    "\n",
    "<img src=\"windy.png\" width=\"400px\">\n",
    "\n",
    "In it, a boat must navigate a 7 &times; 10 gridworld, to reach the goal cell, marked with _G_. There is a crosswind upward through the middle of the grid, in the direction indicated by the gray arrows. The boat has available the standard four actions -- _Up_, _Down_, _Left_ and _Right_. In the region affected by the wind, however, the resulting next state is shifted upward as a consequence of the crosswind, the strength of which varies from column to column. The strength of the wind is given below each column, and corresponds to the number of cells that the movement is shifted upward. For example, if the boat is one cell to the right of the goal, then the action _Left_ takes you to the cell just above the goal.\n",
    "\n",
    "The agent pays a cost of 1 in every step before reaching the goal. The problem can be described as an MDP $(\\mathcal{X},\\mathcal{A},\\mathbf{P},c,\\gamma)$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- MDP problem specification: -\n",
      "\n",
      "States:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " ..., \n",
      " [6 7]\n",
      " [6 8]\n",
      " [6 9]]\n",
      "\n",
      "Actions:\n",
      "['U', 'D', 'L', 'R']\n",
      "\n",
      "Transition probabilities:\n",
      "('Action', 'U')\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "('Action', 'D')\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "('Action', 'L')\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "('Action', 'R')\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "\n",
      "cost:\n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "('\\nStart state:', [3, 0])\n",
      "('\\nGoal state:', [3, 7])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "# Problem specific parameters\n",
    "WIND = (0, 0, 0, 1, 1, 1, 2, 2, 1, 0)\n",
    "nrows = 7\n",
    "ncols = 10\n",
    "init = [3, 0]\n",
    "goal = [3, 7]\n",
    "\n",
    "# States\n",
    "X = [[x, y] for x in range(nrows) for y in range(ncols)]\n",
    "nX = len(X)\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "nA = len(A)\n",
    "\n",
    "# Transition probabilities\n",
    "P = dict()\n",
    "P['U'] = np.zeros((nX, nX))\n",
    "P['D'] = np.zeros((nX, nX))\n",
    "P['L'] = np.zeros((nX, nX))\n",
    "P['R'] = np.zeros((nX, nX))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    x = X[i]\n",
    "    y = dict()\n",
    "    \n",
    "    \n",
    "    y['U'] = [x[0] - WIND[x[1]] - 1, x[1]]\n",
    "    y['D'] = [x[0] - WIND[x[1]] + 1, x[1]]\n",
    "    y['L'] = [x[0] - WIND[x[1]], x[1] - 1]\n",
    "    y['R'] = [x[0] - WIND[x[1]], x[1] + 1]\n",
    "    \n",
    "    for k in y:\n",
    "        y[k][0] = max(min(y[k][0], nrows - 1), 0)\n",
    "        y[k][1] = max(min(y[k][1], ncols - 1), 0)\n",
    "        j = X.index(y[k])\n",
    "        P[k][i, j] = 1\n",
    "\n",
    "c = np.ones((nX, nA))\n",
    "c[X.index(goal), :] = 0\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# -- Pretty print\n",
    "\n",
    "print('\\n- MDP problem specification: -\\n')\n",
    "\n",
    "print('States:')\n",
    "print(np.array(X))\n",
    "\n",
    "print('\\nActions:')\n",
    "print(A)\n",
    "\n",
    "print('\\nTransition probabilities:')\n",
    "for a in A:\n",
    "    print('Action', a)\n",
    "    print(P[a])\n",
    "    \n",
    "print('\\ncost:')\n",
    "print(c)\n",
    "\n",
    "print('\\nStart state:', init)\n",
    "print('\\nGoal state:', goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Compute the optimal _Q_-function for the MDP defined above using value iteration. As your stopping condition, use an error between iterations smaller than `1e-8`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 2107\n",
      "[[ 88.86732306  88.86732306  88.86732306  88.86732306]\n",
      " [ 88.75487178  88.75487178  88.75487178  88.75487178]\n",
      " [ 88.64128462  88.64128462  88.64128462  88.64128462]\n",
      " ..., \n",
      " [ 87.18534608  87.18534608  87.18534608  87.18534608]\n",
      " [ 87.31349262  87.31349262  87.31349262  87.31349262]\n",
      " [ 87.44035769  87.44035769  87.44035769  87.44035769]]\n"
     ]
    }
   ],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "i = 0\n",
    "\n",
    "Q_Optimal = np.zeros((70,4))\n",
    "   \n",
    "err = 1\n",
    "\n",
    "# Q-Function computation\n",
    "\n",
    "#while i < 10:\n",
    "while err > 1e-8:\n",
    "\n",
    "    Q_Up = c[:,0][np.newaxis, :].T + gamma * P[\"U\"].dot(Q_Optimal)\n",
    "    Q_Down = c[:,1][np.newaxis, :].T + gamma * P[\"D\"].dot(Q_Optimal)\n",
    "    Q_Left = c[:,2][np.newaxis, :].T + gamma * P[\"L\"].dot(Q_Optimal)\n",
    "    Q_Right = c[:,3][np.newaxis, :].T + gamma * P[\"R\"].dot(Q_Optimal)\n",
    "        \n",
    "    Qnew = np.min((Q_Up, Q_Down, Q_Left, Q_Right), axis=0)\n",
    "\n",
    "    err = np.linalg.norm(Qnew - Q_Optimal)\n",
    "        \n",
    "\n",
    "    Q_Optimal = Qnew\n",
    "        \n",
    "\n",
    "    i += 1\n",
    "        \n",
    "\n",
    "print \"Iterations:\", i\n",
    "print Q_Optimal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a Python function that, given a Q-function $Q$ and a state $x$, selects a random action using the $\\epsilon$-greedy policy obtained from $Q$ for state $x$. Your function should receive an optional parameter, corresponding to $\\epsilon$, with default value of 0.1. \n",
    "\n",
    "**Note:** In the case of two actions with the same value, your $\\epsilon$-greedy policy should randomize between the two.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert your code here.\n",
    "import numpy as np\n",
    "import random\n",
    "#t = np.array([[1,1,1]])\n",
    "\n",
    "def select_action(Q,nX,e_value = None):\n",
    "    if e_value is None:\n",
    "        e_value = 0.1\n",
    "    \n",
    "    #print \"e:\", e_value\n",
    "    \n",
    "    state_index = X.index(nX)\n",
    "\n",
    "    # primeiro verifica qual é o min da lista depois verifica se existem mais do que 1 min\n",
    "    # nesse caso vai guardar os indices desses valores para depois escolher um aleatoriamente\n",
    "    \n",
    "    aux=[]\n",
    "    for i,e in enumerate(Q[state_index]):\n",
    "        if e == np.amin(Q[state_index]):\n",
    "            aux += [i] \n",
    "\n",
    "    \n",
    "    if len(aux)>1:\n",
    "        action_index = random.choice(aux)\n",
    "    else:\n",
    "        action_index = aux[0]      \n",
    "    \n",
    "    \n",
    "    # pega no indice obtido e escolhe a acçao correspondente\n",
    "    selected_action = A[action_index]\n",
    "    \n",
    "    return selected_action\n",
    "\n",
    "    \n",
    "select_action(Q_Optimal,[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Model-based learning\n",
    "\n",
    "You will now run the model-based learning algorithm discussed in class, and evaluate its learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Run the model-based reinforcement learning algorithm discussed in class to compute $Q^*$ for 100,000 iterations. Initialize each transition probability matrix as the identity and the cost function as all-zeros. Use an $\\epsilon$-greedy policy with $\\epsilon=0.1$ (use the function from Activity 2). Note that, at each step,\n",
    "\n",
    "* You will need to select an action according to the $\\epsilon$-greedy policy;\n",
    "* The state and action, you will then compute the cost and generate the next state; \n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update. \n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "In order to ensure that your algorithm visits every state and action a sufficient number of times, after the boat reaches the goal cell, make one further step, the corresponding update, and then reset the position of the boat to a random state in the environment.\n",
    "\n",
    "Plot the norm $\\|Q^*-Q^{(k)}\\|$ every 500 iterations of your method, where $Q^*$ is the optimal _Q_~function computed in Activity 1.\n",
    "\n",
    "**Note:** The simulation may take a bit. Don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1475.81408634\n",
      "Norm: 1261.51655602\n",
      "Norm: 1166.06546153\n",
      "Norm: 1131.77120753\n",
      "Norm: 1120.86457498\n",
      "Norm: 1065.52253286\n",
      "Norm: 1038.73022758\n",
      "Norm: 1010.35542666\n",
      "Norm: 796.313679259\n",
      "Norm: 447.815451957\n",
      "Norm: 260.776532794\n",
      "Norm: 262.18982922\n",
      "Norm: 262.199798665\n",
      "Norm: 262.199864198\n",
      "Norm: 262.199864629\n",
      "Norm: 262.199864632\n",
      "Norm: 262.199864632\n",
      "Norm: 262.199864632\n",
      "Norm: 262.199864632\n",
      "Norm: 262.199864632\n",
      "Iterations: 10000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "# Inicializar probabilidades de transiçao a 0\n",
    "P_Up = np.eye(nX, nX)\n",
    "P_Down = np.eye(nX, nX)\n",
    "P_Left = np.eye(nX, nX)\n",
    "P_Right = np.eye(nX, nX)\n",
    "\n",
    "# Inicializar custos a 0\n",
    "cost = np.zeros((nX, nA))\n",
    "\n",
    "state_visits = np.zeros((70,4))\n",
    "\n",
    "\n",
    "def calc_step_size(current_state, action):\n",
    "    return 1 / ( state_visits[X.index(current_state), A.index(action)] + 1 )\n",
    "    \n",
    "\n",
    "def update_state(current_state, action):\n",
    "    \n",
    "# AS COORDENADAS DO PROF ESTAO COMO [Y,X] for some reason ...\n",
    "\n",
    "    if action == \"U\":\n",
    "        \n",
    "        if WIND[current_state[1]]==0:\n",
    "            new_state = [current_state[0] + 1, current_state[1]]\n",
    "        elif WIND[current_state[1]]==1:\n",
    "            new_state = [current_state[0] + 2, current_state[1]]\n",
    "        else:\n",
    "            new_state = [current_state[0] + 3, current_state[1]]\n",
    "        \n",
    "        if new_state[0]>6:\n",
    "            new_state[0]=6\n",
    "            \n",
    "        \n",
    "    elif action == \"D\":\n",
    "        \n",
    "        if WIND[current_state[1]]==0:\n",
    "            new_state = [current_state[0] - 1, current_state[1]]\n",
    "        elif WIND[current_state[1]]==1:    \n",
    "            new_state = [current_state[0], current_state[1]]\n",
    "        else:\n",
    "            new_state = [current_state[0] + 1, current_state[1]]\n",
    "        \n",
    "        if new_state[0]>6:\n",
    "            new_state[0]=6\n",
    "        if new_state[0]<0:\n",
    "            new_state[0]=0\n",
    "            \n",
    "    elif action == \"L\":\n",
    "      \n",
    "        if WIND[current_state[1]]==0:\n",
    "            new_state = [current_state[0], current_state[1] - 1]\n",
    "        elif WIND[current_state[1]]==1:\n",
    "            new_state = [current_state[0] + 1, current_state[1] - 1]\n",
    "        else:\n",
    "            new_state = [current_state[0] + 2, current_state[1] - 1]\n",
    "            \n",
    "        if new_state[0]>6:\n",
    "            new_state[0]=6\n",
    "        if new_state[1]<0:\n",
    "            new_state[1]=0\n",
    "        \n",
    "    elif action == \"R\":\n",
    "        if WIND[current_state[1]]==0:\n",
    "            new_state = [current_state[0], current_state[1] + 1]\n",
    "        elif WIND[current_state[1]]==1:\n",
    "            new_state = [current_state[0] + 1, current_state[1] + 1]\n",
    "        else:\n",
    "            new_state = [current_state[0] + 2, current_state[1] + 1]\n",
    "            \n",
    "        if new_state[0]>6:\n",
    "            new_state[0]=6\n",
    "        if new_state[1]>9:\n",
    "            new_state[1]=9\n",
    "    \n",
    "    # state_visits[estado, acçao]\n",
    "    \n",
    "    #print \"next state antes do if:\", new_state\n",
    "    \n",
    "\n",
    "    return new_state\n",
    "    \n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "\n",
    "i = 0\n",
    "\n",
    "Q_k = np.zeros((70,4))\n",
    "\n",
    "goal_reached = False\n",
    "\n",
    "current_state = [0, 0] # TEMOS DE SUBSTITUIR PELO ESTADO INICIAL !!!\n",
    "\n",
    "while i < 10000: # demora uns 30 segundos a correr 100000 iteraçoes\n",
    "\n",
    "    Q_Up = cost[:,0][np.newaxis, :].T + gamma * P_Up.dot(Q_k)\n",
    "    Q_Down = cost[:,1][np.newaxis, :].T + gamma * P_Down.dot(Q_k)\n",
    "    Q_Left = cost[:,2][np.newaxis, :].T + gamma * P_Left.dot(Q_k)\n",
    "    Q_Right = cost[:,3][np.newaxis, :].T + gamma * P_Right.dot(Q_k)\n",
    "        \n",
    "    Qnew = np.min((Q_Up, Q_Down, Q_Left, Q_Right), axis=0)\n",
    "        \n",
    "    Q_k = Qnew\n",
    "    \n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print \"Norm:\", np.linalg.norm(Q_Optimal - Q_k)\n",
    "    \n",
    "    \n",
    "    #print \"------------------------------------\"\n",
    "    #print \"iteration:\",i\n",
    "    #print \"current state:\", current_state\n",
    "    \n",
    "\n",
    "    action = select_action(Q_k,current_state)\n",
    "    \n",
    "    step_size = calc_step_size(current_state, action)\n",
    "\n",
    "    #print \"action:\", action\n",
    "    \n",
    "    cost[X.index(current_state), A.index(action)] = cost[X.index(current_state), A.index(action)] + step_size * (c[X.index(current_state), A.index(action)] - cost[X.index(current_state), A.index(action)])\n",
    "    \n",
    "    #print cost\n",
    "    \n",
    "    if goal_reached == True:\n",
    "        \n",
    "        #print \"JA PASSAMOS PELO GOAL\"\n",
    "        \n",
    "        next_state = random.choice(X)\n",
    "        \n",
    "        state_visits[X.index(current_state), A.index(action)] += 1\n",
    "    \n",
    "        #print state_visits\n",
    "    \n",
    "        #print \"next state:\", next_state\n",
    "        \n",
    "        current_state = next_state\n",
    "        \n",
    "    else:\n",
    "         \n",
    "\n",
    "        next_state = update_state(current_state, action)\n",
    "\n",
    "        #print \"next state fora do update:\", next_state\n",
    "\n",
    "        state_visits[X.index(current_state), A.index(action)] += 1\n",
    "\n",
    "        #print state_visits\n",
    "\n",
    "        #print \"next state:\", next_state\n",
    "        #print \"step size:\", step_size\n",
    "        #print state_visits\n",
    "        #print \"\\n\"\n",
    "\n",
    "        current_state = next_state  \n",
    "    \n",
    "    if current_state == goal:\n",
    "        goal_reached = True\n",
    "        \n",
    "        \n",
    "    i += 1\n",
    "        \n",
    "\n",
    "print \"Iterations:\", i\n",
    "print \"done\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Temporal-difference learning\n",
    "\n",
    "You will now run both Q-learning and SARSA, and compare their learning performance with that of the model-based method just studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Repeat Activity 3 but using the _Q_-learning algorithm with a learning rate $\\alpha=0.3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "# Inicializar probabilidades de transiçao a 0\n",
    "P_Up = np.eye(nX, nX)\n",
    "P_Down = np.eye(nX, nX)\n",
    "P_Left = np.eye(nX, nX)\n",
    "P_Right = np.eye(nX, nX)\n",
    "\n",
    "# Inicializar custos a 0\n",
    "cost = np.zeros((nX, nA))\n",
    "\n",
    "state_visits = np.zeros((70,4))\n",
    "\n",
    "i = 0\n",
    "\n",
    "goal_reached = False\n",
    "\n",
    "current_state = [0, 0] # TEMOS DE SUBSTITUIR PELO ESTADO INICIAL !!!\n",
    "\n",
    "while i < 1000: # demora uns 30 segundos a correr 100000 iteraçoes\n",
    "    \n",
    "    # Qtx+1(Xt,At) = Qt(Xt,At) + alpha * [Ct + gamma*min(Qt(Xt+1,a')) - Qt(Xt,At)]\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 4 but using the SARSA algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Insert your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "Discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "_Add your discussion here._"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
