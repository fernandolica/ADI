{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 3: Markov decision problems\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;.\n",
    "\n",
    "### 1. Modeling\n",
    "\n",
    "Consider once again the predator-prey domain described in the Homework and which you described as a Markov decision process.\n",
    "\n",
    "<img src=\"toroidal-world.png\" width=\"400px\">\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* toroidal world \"wraps around\", i.e., an individual exiting through any of the four sides of the grid reenters on the opposite side (see figure above).\n",
    "\n",
    "* At each time step, the hare selects uniformly at random one of the four directions (up, down, left, and right) and moves to the adjacent cell in that direction with a probability 0.4. With a probability 0.6 it remains in the same cell. \n",
    "\n",
    "* The wolf, on the other hand, can select at each time step one of five actions---up (_U_), down (_D_), left (_L_) and right (_R_) or stay (_S_). If it selects action _S_, it remains in the same cell with probability 1.0. Otherwise, the other 4 actions succeed in moving the wolf to the adjacent cell in the corresponding direction with a probability 0.8 and fail with a probability 0.2. \n",
    "\n",
    "* The goal of the wolf is to catch the hare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your Markov decision process in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* Define a `numpy`array with the costs\n",
    "\n",
    "The order for the states and actions used in the transition probability and cost matrices should match that in the lists of states and actions. \n",
    "\n",
    "**Note 1**: Don't forget to import `numpy`.\n",
    "\n",
    "**Note 2**: You can define the transition probability matrices for each of the two individuals and then build the combined transition probability matrices using the `numpy.kron` function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "states= [(1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4), (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4)]\n",
    "actions = [\"Up\" , \"Down\", \"Left\", \"Right\", \"Stay\"]\n",
    "\n",
    "\n",
    "P_Up = np.array([[0.12,0.04,0.04,0.  ,0.  ,0.  ,0.  ,0.  ,0.48, 0.16,0.16,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.04,0.12,0.  ,0.04,0.  ,0.  ,0.  ,0.  ,0.16, 0.48,0.  ,0.16,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.04,0.  ,0.12,0.04,0.  ,0.  ,0.  ,0.  ,0.16, 0.  ,0.48,0.16,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.  ,0.04,0.04,0.12,0.  ,0.  ,0.  ,0.  ,0.  , 0.16,0.16,0.48,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.12,0.04,0.04,0.  ,0.  , 0.  ,0.  ,0.  ,0.48,0.16,0.16,0.  ],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.04,0.12,0.  ,0.04,0.  , 0.  ,0.  ,0.  ,0.16,0.48,0.  ,0.16],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.04,0.  ,0.12,0.04,0.  , 0.  ,0.  ,0.  ,0.16,0.  ,0.48,0.16],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.  ,0.04,0.04,0.12,0.  , 0.  ,0.  ,0.  ,0.  ,0.16,0.16,0.48],\n",
    "                 [0.48,0.16,0.16,0.  ,0.  ,0.  ,0.  ,0.  ,0.12, 0.04,0.04,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.16,0.48,0.  ,0.16,0.  ,0.  ,0.  ,0.  ,0.04, 0.12,0.  ,0.04,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.16,0.  ,0.48,0.16,0.  ,0.  ,0.  ,0.  ,0.04, 0.  ,0.12,0.04,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.  ,0.16,0.16,0.48,0.  ,0.  ,0.  ,0.  ,0.  , 0.04,0.04,0.12,0.  ,0.  ,0.  ,0.  ],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.48,0.16,0.16,0.  ,0.  , 0.  ,0.  ,0.  ,0.12,0.04,0.04,0.  ],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.16,0.48,0.  ,0.16,0.  , 0.  ,0.  ,0.  ,0.04,0.12,0.  ,0.04],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.16,0.  ,0.48,0.16,0.  , 0.  ,0.  ,0.  ,0.04,0.  ,0.12,0.04],\n",
    "                 [0.  ,0.  ,0.  ,0.  ,0.  ,0.16,0.16,0.48,0.  , 0.  ,0.  ,0.  ,0.  ,0.04,0.04,0.12]])\n",
    "\n",
    "P_Down = np.array([[0.12,0.04,0.04,0.  ,0.  ,0.  ,0.  ,0.  ,0.48, 0.16,0.16,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.04,0.12,0.  ,0.04,0.  ,0.  ,0.  ,0.  ,0.16, 0.48,0.  ,0.16,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.04,0.  ,0.12,0.04,0.  ,0.  ,0.  ,0.  ,0.16, 0.  ,0.48,0.16,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.04,0.04,0.12,0.  ,0.  ,0.  ,0.  ,0.  , 0.16,0.16,0.48,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.12,0.04,0.04,0.  ,0.  , 0.  ,0.  ,0.  ,0.48,0.16,0.16,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.04,0.12,0.  ,0.04,0.  , 0.  ,0.  ,0.  ,0.16,0.48,0.  ,0.16],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.04,0.  ,0.12,0.04,0.  , 0.  ,0.  ,0.  ,0.16,0.  ,0.48,0.16],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.04,0.04,0.12,0.  , 0.  ,0.  ,0.  ,0.  ,0.16,0.16,0.48],\n",
    "                   [0.48,0.16,0.16,0.  ,0.  ,0.  ,0.  ,0.  ,0.12, 0.04,0.04,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.16,0.48,0.  ,0.16,0.  ,0.  ,0.  ,0.  ,0.04, 0.12,0.  ,0.04,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.16,0.  ,0.48,0.16,0.  ,0.  ,0.  ,0.  ,0.04, 0.  ,0.12,0.04,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.16,0.16,0.48,0.  ,0.  ,0.  ,0.  ,0.  , 0.04,0.04,0.12,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.48,0.16,0.16,0.  ,0.  , 0.  ,0.  ,0.  ,0.12,0.04,0.04,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.16,0.48,0.  ,0.16,0.  , 0.  ,0.  ,0.  ,0.04,0.12,0.  ,0.04],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.16,0.  ,0.48,0.16,0.  , 0.  ,0.  ,0.  ,0.04,0.  ,0.12,0.04],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.16,0.16,0.48,0.  , 0.  ,0.  ,0.  ,0.  ,0.04,0.04,0.12]])\n",
    "\n",
    "P_Left = np.array([[0.12,0.04,0.04,0.  ,0.48,0.16,0.16,0.  ,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.04,0.12,0.  ,0.04,0.16,0.48,0.  ,0.16,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.04,0.  ,0.12,0.04,0.16,0.  ,0.48,0.16,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.04,0.04,0.12,0.  ,0.16,0.16,0.48,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.48,0.16,0.16,0.  ,0.12,0.04,0.04,0.  ,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.16,0.48,0.  ,0.16,0.04,0.12,0.  ,0.04,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.16,0.  ,0.48,0.16,0.04,0.  ,0.12,0.04,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.16,0.16,0.48,0.  ,0.04,0.04,0.12,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.12, 0.04,0.04,0.  ,0.48,0.16,0.16,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.04, 0.12,0.  ,0.04,0.16,0.48,0.  ,0.16],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.04, 0.  ,0.12,0.04,0.16,0.  ,0.48,0.16],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  , 0.04,0.04,0.12,0.  ,0.16,0.16,0.48],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.48, 0.16,0.16,0.  ,0.12,0.04,0.04,0.  ],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.16, 0.48,0.  ,0.16,0.04,0.12,0.  ,0.04],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.16, 0.  ,0.48,0.16,0.04,0.  ,0.12,0.04],\n",
    "                   [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  , 0.16,0.16,0.48,0.  ,0.04,0.04,0.12]])\n",
    "\n",
    "P_Right = np.array([[0.12,0.04,0.04,0.  ,0.48,0.16,0.16,0.  ,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.04,0.12,0.  ,0.04,0.16,0.48,0.  ,0.16,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.04,0.  ,0.12,0.04,0.16,0.  ,0.48,0.16,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.  ,0.04,0.04,0.12,0.  ,0.16,0.16,0.48,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.48,0.16,0.16,0.  ,0.12,0.04,0.04,0.  ,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.16,0.48,0.  ,0.16,0.04,0.12,0.  ,0.04,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.16,0.  ,0.48,0.16,0.04,0.  ,0.12,0.04,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.  ,0.16,0.16,0.48,0.  ,0.04,0.04,0.12,0.  , 0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.12, 0.04,0.04,0.  ,0.48,0.16,0.16,0.  ],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.04, 0.12,0.  ,0.04,0.16,0.48,0.  ,0.16],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.04, 0.  ,0.12,0.04,0.16,0.  ,0.48,0.16],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  , 0.04,0.04,0.12,0.  ,0.16,0.16,0.48],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.48, 0.16,0.16,0.  ,0.12,0.04,0.04,0.  ],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.16, 0.48,0.  ,0.16,0.04,0.12,0.  ,0.04],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.16, 0.  ,0.48,0.16,0.04,0.  ,0.12,0.04],\n",
    "                    [0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  ,0.  , 0.16,0.16,0.48,0.  ,0.04,0.04,0.12]])\n",
    "\n",
    "P_Stay = np.array([[ 0.6,0.2,0.2,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0.2,0.6,0. ,0.2,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0.2,0. ,0.6,0.2,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0.2,0.2,0.6,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0.6,0.2,0.2,0. ,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0.2,0.6,0. ,0.2,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0.2,0. ,0.6,0.2,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0.2,0.2,0.6,0. ,0. ,0. , 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0.6,0.2,0.2, 0. ,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0.2,0.6,0. , 0.2,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0.2,0. ,0.6, 0.2,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0.2,0.2, 0.6,0. ,0. ,0. ,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0.6,0.2,0.2,0. ],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0.2,0.6,0. ,0.2],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0.2,0. ,0.6,0.2],\n",
    "                   [ 0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. ,0. , 0. ,0. ,0.2,0.2,0.6]])\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###############  Matrix de Custo   ################################\n",
    "###################################################################\n",
    "\n",
    "same_position_state_indexes = [0, 5, 10, 15]\n",
    "\n",
    "above_bellow_position_state_indexes = [2, 7, 8, 13]\n",
    "\n",
    "left_right_position_state_indexes = [1, 4, 11, 14]\n",
    "\n",
    "diagonal_position_state_indexes = [3, 6, 9, 12]\n",
    "\n",
    "stay = 4\n",
    "\n",
    "up_or_down = [0, 1]\n",
    "\n",
    "left_or_right = [2, 3]\n",
    "\n",
    "def build_cost_matrix():\n",
    "    costMatrix = np.zeros((16, 5))\n",
    "    for possible_action in range(5):\n",
    "        for possible_state in range(16):\n",
    "            if possible_state in same_position_state_indexes and possible_action == stay:\n",
    "                costMatrix[possible_state, possible_action] = 0\n",
    "                \n",
    "            elif possible_state in above_bellow_position_state_indexes and possible_action in up_or_down:\n",
    "                costMatrix[possible_state, possible_action] = 0\n",
    "            \n",
    "            elif possible_state in left_right_position_state_indexes and possible_action in left_or_right:\n",
    "                costMatrix[possible_state, possible_action] = 0\n",
    "                \n",
    "            elif possible_state in diagonal_position_state_indexes and possible_action != stay:\n",
    "                costMatrix[possible_state, possible_action] = 0\n",
    "                \n",
    "            else:\n",
    "                costMatrix[possible_state, possible_action] = 1\n",
    "                \n",
    "    return costMatrix\n",
    "\n",
    "build_cost_matrix()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediction\n",
    "\n",
    "You are now going to evaluate a given policy, computing the corresponding cost-to-go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Describe the policy that, in each state $(w, h)$, always moves the wolf to the cell closest to the hare. If multiple such cells exist, the wolf should select randomly between the two.\n",
    "\n",
    "For example, suppose that the wolf is in cell 1 and the hare in cell 4 (figure above, left). The wolf should then select randomly between the actions _U_, _D_ (which move the wolf to cell 3), _L_ and _R_ (which move the wolf to cell 2). Conversely, if the wolf is in cell 1 and the hare in cell 3 (figure above, right), the wolf should select randomly between the two actions _U_ and _D_ (which move the wolf to cell 3).\n",
    "\n",
    "**Note:** The policy should be described as a vector with as many rows as there are states and as many columns as there are actions, where the entry _xa_ has the probability of selecting action _a_ in state _x_.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.5 ,  0.5 ,  0.  ],\n",
       "       [ 0.5 ,  0.5 ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.5 ,  0.5 ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25,  0.  ],\n",
       "       [ 0.5 ,  0.5 ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.5 ,  0.5 ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.5 ,  0.5 ,  0.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25,  0.  ],\n",
       "       [ 0.5 ,  0.5 ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.5 ,  0.5 ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  1.  ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "same_position_state_indexes = [0, 5, 10, 15]\n",
    "\n",
    "above_bellow_position_state_indexes = [2, 7, 8, 13]\n",
    "\n",
    "left_right_position_state_indexes = [1, 4, 11, 14]\n",
    "\n",
    "diagonal_position_state_indexes = [3, 6, 9, 12]\n",
    "\n",
    "stay = 4\n",
    "\n",
    "up_or_down = [0, 1]\n",
    "\n",
    "left_or_right = [2, 3]\n",
    "\n",
    "def build_policy_matrix():\n",
    "    policyMatrix = np.zeros((16, 5))\n",
    "    for possible_action in range(5):\n",
    "        for possible_state in range(16):\n",
    "            if possible_state in same_position_state_indexes and possible_action == stay:\n",
    "                policyMatrix[possible_state, possible_action] = 1\n",
    "                \n",
    "            elif possible_state in above_bellow_position_state_indexes and possible_action in up_or_down:\n",
    "                policyMatrix[possible_state, possible_action] = 0.5\n",
    "            \n",
    "            elif possible_state in left_right_position_state_indexes and possible_action in left_or_right:\n",
    "                policyMatrix[possible_state, possible_action] = 0.5\n",
    "                \n",
    "            elif possible_state in diagonal_position_state_indexes and possible_action != stay:\n",
    "                policyMatrix[possible_state, possible_action] = 0.25\n",
    "                \n",
    "            else:\n",
    "                policyMatrix[possible_state, possible_action] = 0\n",
    "                \n",
    "    return policyMatrix\n",
    "\n",
    "build_policy_matrix()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "Compute the cost-to-go function $J^\\pi$ associated with the policy from Activity 2. Use $\\gamma=0.99$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.888],\n",
       "       [ 44.997],\n",
       "       [ 44.997],\n",
       "       [ 45.515],\n",
       "       [ 44.997],\n",
       "       [ 43.888],\n",
       "       [ 45.515],\n",
       "       [ 44.997],\n",
       "       [ 44.997],\n",
       "       [ 45.515],\n",
       "       [ 43.888],\n",
       "       [ 44.997],\n",
       "       [ 45.515],\n",
       "       [ 44.997],\n",
       "       [ 44.997],\n",
       "       [ 43.888]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "ident = np.eye(16)\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "pi = build_policy_matrix() # politica inicial\n",
    "\n",
    "# USAMOS ESTA OU PODEMOS USAR A NOSSA ?\n",
    "costMatrix = np.array([[0,0,0,0,0],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],\n",
    "                       [0,0,0,0,0],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],\n",
    "                       [0,0,0,0,0],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],\n",
    "                       [0,0,0,0,0]])\n",
    "\n",
    "def calc_cost_to_go():\n",
    "    \n",
    "    c_Up = costMatrix[:,0][np.newaxis, :].T\n",
    "    c_Down = costMatrix[:,0][np.newaxis, :].T\n",
    "    c_Left = costMatrix[:,2][np.newaxis, :].T\n",
    "    c_Right = costMatrix[:,2][np.newaxis, :].T\n",
    "    c_Stay = costMatrix[:,4][np.newaxis, :].T\n",
    "    \n",
    "        \n",
    "    p_Up = P_Up \n",
    "    p_Down = P_Down \n",
    "    p_Left = P_Left\n",
    "    p_Right = P_Right\n",
    "    p_Stay = P_Stay\n",
    "    \n",
    "\n",
    "    \n",
    "    Ppi = np.diag(pi[:,0]).dot(p_Up) + \\\n",
    "          np.diag(pi[:,1]).dot(p_Down) + \\\n",
    "          np.diag(pi[:,2]).dot(p_Left) + \\\n",
    "          np.diag(pi[:,3]).dot(p_Right) + \\\n",
    "          np.diag(pi[:,4]).dot(p_Stay)\n",
    "    \n",
    "    Cpi = np.diag(pi[:,0]).dot(c_Up) + \\\n",
    "          np.diag(pi[:,1]).dot(c_Down) + \\\n",
    "          np.diag(pi[:,2]).dot(c_Left) + \\\n",
    "          np.diag(pi[:,3]).dot(c_Right) + \\\n",
    "          np.diag(pi[:,4]).dot(c_Stay)\n",
    "                \n",
    "    \n",
    "    J = np.linalg.inv(np.eye(16) - gamma * Ppi).dot(Cpi)\n",
    "    \n",
    "    return J\n",
    "        \n",
    "    \n",
    "#######################\n",
    "calc_cost_to_go()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Control\n",
    "\n",
    "In this section you are going to compare value and policy iteration, both in terms of time and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Show that the policy in Activity 3 is optimal: use value iteration to compute $J^*$ and show that $J^*=J^\\pi$. Track the time and the number of iterations taken to compute $J^*$.\n",
    "\n",
    "**Note 1:** Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note 2:** You may find useful the function ``time()`` from the module ``time``.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1891\n",
      "\n",
      "\n",
      "Tempo: 0.176277538955\n",
      "\n",
      "\n",
      "[[ 43.888]\n",
      " [ 44.997]\n",
      " [ 44.997]\n",
      " [ 45.515]\n",
      " [ 44.997]\n",
      " [ 43.888]\n",
      " [ 45.515]\n",
      " [ 44.997]\n",
      " [ 44.997]\n",
      " [ 45.515]\n",
      " [ 43.888]\n",
      " [ 44.997]\n",
      " [ 45.515]\n",
      " [ 44.997]\n",
      " [ 44.997]\n",
      " [ 43.888]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "policy_J_optimal = np.zeros((16,1))\n",
    "\n",
    "\n",
    "\n",
    "def calc_val_iter():\n",
    "    \n",
    "    # o [np.newaxis, :].T e uma forma estranha de transpor porque o .T apenas nao tava a dar\n",
    "    c_UpDown = costMatrix[:,0][np.newaxis, :].T\n",
    "    c_LeftRight = costMatrix[:,2][np.newaxis, :].T\n",
    "    c_Stay = costMatrix[:,4][np.newaxis, :].T\n",
    "    \n",
    "    p_UpDown = P_Up\n",
    "    p_LeftRight = P_Left\n",
    "    p_Stay = P_Stay\n",
    "\n",
    "    gamma = 0.99\n",
    "\n",
    "    J = np.zeros((16,1))\n",
    "    err = 1\n",
    "    \n",
    "    while err > 1e-8:\n",
    "        \n",
    "\n",
    "        q_UpDown = c_UpDown + gamma * p_UpDown.dot(J)\n",
    "        q_LeftRight = c_LeftRight + gamma * p_LeftRight.dot(J)\n",
    "        q_Stay = c_Stay + gamma * p_Stay.dot(J)\n",
    "\n",
    "       \n",
    "        Jnew = np.min((q_UpDown, q_LeftRight, q_Stay), axis=0)\n",
    "\n",
    "        err = np.linalg.norm(Jnew - J)\n",
    "        \n",
    "        global iteration\n",
    "        iteration += 1\n",
    "        J = Jnew\n",
    "    \n",
    "    \n",
    "    \n",
    "    global policy_J_optimal\n",
    "    policy_J_optimal = J\n",
    "################################\n",
    "\n",
    "# TEM DE DEVOLVER UM VECTOR\n",
    " \n",
    "calc_val_iter()   \n",
    "print(\"Iterations: \"+ str(iteration))\n",
    "print(\"\\n\")\n",
    "print(\"Tempo: \" + str(timeit.timeit(calc_val_iter, number=1)))\n",
    "print(\"\\n\")\n",
    "print(policy_J_optimal)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "Compute once again the optimal policy now using policy iteration. Track the time and number of iterations taken and compare to those of Activity 4.\n",
    "\n",
    "**Note:** If you find that numerical errors affect your computations (especially when comparing two values/arrays) you may use the `numpy` function `isclose` with adequately set absolute and relative tolerance parameters (e.g., $10^{-8}$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1\n",
      "\n",
      "\n",
      "Tempo: 0.00173200976426\n",
      "\n",
      "\n",
      "[[ 0.    0.    0.    0.    1.  ]\n",
      " [ 0.    0.    0.5   0.5   0.  ]\n",
      " [ 0.5   0.5   0.    0.    0.  ]\n",
      " [ 0.25  0.25  0.25  0.25  0.  ]\n",
      " [ 0.    0.    0.5   0.5   0.  ]\n",
      " [ 0.    0.    0.    0.    1.  ]\n",
      " [ 0.25  0.25  0.25  0.25  0.  ]\n",
      " [ 0.5   0.5   0.    0.    0.  ]\n",
      " [ 0.5   0.5   0.    0.    0.  ]\n",
      " [ 0.25  0.25  0.25  0.25  0.  ]\n",
      " [ 0.    0.    0.    0.    1.  ]\n",
      " [ 0.    0.    0.5   0.5   0.  ]\n",
      " [ 0.25  0.25  0.25  0.25  0.  ]\n",
      " [ 0.5   0.5   0.    0.    0.  ]\n",
      " [ 0.    0.    0.5   0.5   0.  ]\n",
      " [ 0.    0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "policy_J_optimal = np.zeros((16,16))\n",
    "\n",
    "def calc_pol_iter():\n",
    "    \n",
    "    c_Up = costMatrix[:,0]\n",
    "    c_Down = costMatrix[:,0]\n",
    "    c_Left = costMatrix[:,2]\n",
    "    c_Right = costMatrix[:,2]\n",
    "    c_Stay = costMatrix[:,4]\n",
    "        \n",
    "    p_Up = P_Up \n",
    "    p_Down = P_Down \n",
    "    p_Left = P_Left\n",
    "    p_Right = P_Right\n",
    "    p_Stay = P_Stay\n",
    "\n",
    "    gamma = 0.99\n",
    "    \n",
    "    pi = build_policy_matrix() # politica inicial\n",
    "    \n",
    "    quit = False \n",
    "\n",
    "    #\"up\",\"down\",\"left\",\"right,\"stay\"\n",
    "    \n",
    "    while not quit:\n",
    "        \n",
    "\n",
    "        Cpi = np.diag(pi[:,0]).dot(c_Up) + \\\n",
    "              np.diag(pi[:,1]).dot(c_Down) + \\\n",
    "              np.diag(pi[:,2]).dot(c_Left) + \\\n",
    "              np.diag(pi[:,3]).dot(c_Right) + \\\n",
    "              np.diag(pi[:,4]).dot(c_Stay)\n",
    "                \n",
    "        Ppi = np.diag(pi[:,0]).dot(p_Up) + \\\n",
    "              np.diag(pi[:,1]).dot(p_Down) + \\\n",
    "              np.diag(pi[:,2]).dot(p_Left) + \\\n",
    "              np.diag(pi[:,3]).dot(p_Right) + \\\n",
    "              np.diag(pi[:,4]).dot(p_Stay)\n",
    "                \n",
    "        J = np.linalg.inv(np.eye(16) - gamma * Ppi).dot(Cpi)\n",
    "        \n",
    "        \n",
    "        \n",
    "        Qup = c_Up + gamma * p_Up.dot(J)\n",
    "        Qdown = c_Down + gamma * p_Down.dot(J)\n",
    "        Qleft = c_Left + gamma * p_Left.dot(J)\n",
    "        Qright = c_Right + gamma * p_Right.dot(J)\n",
    "        Qstay = c_Stay + gamma * p_Stay.dot(J)\n",
    "        \n",
    "        \n",
    "        pinew = np.zeros((16,5))\n",
    "        \n",
    "        \n",
    "        pinew[:,0] = np.isclose(Qup, np.min([Qup, Qdown, Qleft, Qright, Qstay], axis=0), atol=1e-10, rtol=1e-10).astype(int)\n",
    "        pinew[:,1] = np.isclose(Qdown, np.min([Qup, Qdown, Qleft, Qright, Qstay], axis=0), atol=1e-10, rtol=1e-10).astype(int)\n",
    "        pinew[:,2] = np.isclose(Qleft, np.min([Qup, Qdown, Qleft, Qright, Qstay], axis=0), atol=1e-10, rtol=1e-10).astype(int)\n",
    "        pinew[:,3] = np.isclose(Qright, np.min([Qup, Qdown, Qleft, Qright, Qstay], axis=0), atol=1e-10, rtol=1e-10).astype(int)\n",
    "        pinew[:,4] = np.isclose(Qstay, np.min([Qup, Qdown, Qleft, Qright, Qstay], axis=0), atol=1e-10, rtol=1e-10).astype(int)\n",
    "    \n",
    "        pinew = pinew / np.sum(pinew, axis=1, keepdims = True)\n",
    "        \n",
    "        quit = (pi == pinew).all()\n",
    "        pi = pinew\n",
    "        \n",
    "        global iteration\n",
    "        iteration += 1\n",
    "    \n",
    "    global policy_J_optimal\n",
    "    policy_J_optimal = pi\n",
    "    \n",
    "calc_pol_iter()\n",
    "print(\"Iterations: \"+ str(iteration))\n",
    "print(\"\\n\")\n",
    "print(\"Tempo: \" + str(timeit.timeit(calc_pol_iter, number=1)))\n",
    "print(\"\\n\")\n",
    "print(policy_J_optimal)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulation\n",
    "\n",
    "Finally, in this section you will check whether the theoretical computations of the cost-to-go actually correspond to the cost incurred by an agent following a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Starting in each of the two states $x$ in the initial figure, \n",
    "\n",
    "* Generate **100** trajectories of 10,000 steps each, following the optimal policy for the MDP. \n",
    "* For each trajectory, compute the accumulated (discounted) cost. \n",
    "* Compute the average cost over the 100 trajectories.\n",
    "* Compare the resulting value with that computed in Activity 4 for the two states. \n",
    "\n",
    "** Note:** The simulation may take a bit of time, don't despair ☺️.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 4), (1, 3), (1, 3), (3, 3), (4, 4), (4, 4), (4, 4), (4, 4), (3, 3), (3, 1)]\n",
      "\n",
      "\n",
      "[(1, 4), (2, 4), (4, 4), (4, 4), (4, 4), (4, 4), (2, 2), (2, 2), (4, 4), (4, 4), (2, 2)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy.random as rnd \n",
    "\n",
    "\n",
    "#print states\n",
    "#print actions\n",
    "#print policy_J_optimal \n",
    "\n",
    "trajectories = []\n",
    "steps = []\n",
    "discountedCost= []\n",
    "\n",
    "t=0 #100\n",
    "s=0 #10000\n",
    "\n",
    "##probailites when choosing an action\n",
    "wolfP =[0.8, 0.2] #[move, stay]\n",
    "hareP =[0.4, 0.6] #[move, stay]\n",
    "a= [\"M\" , \"S\"]\n",
    "\n",
    "def moveUpDown(actualpos):\n",
    "    return{\n",
    "        1: 3,\n",
    "        2: 4,\n",
    "        3: 1,\n",
    "        4: 2,\n",
    "    }[actualpos]\n",
    "\n",
    "\n",
    "def moveLeftRight(actualpos):\n",
    "    return{\n",
    "        1: 2,\n",
    "        2: 1,\n",
    "        3: 4,\n",
    "        4: 3,\n",
    "    }[actualpos]\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "while t<2:  #100\n",
    "    \n",
    "    init = (1,4)\n",
    "    s=0\n",
    "    nextPos = (1, 4)\n",
    "    trajectory=[nextPos]\n",
    "    while s<10: # 10000\n",
    "        \n",
    "        #print nextPos\n",
    "        \n",
    "        #print \"===hare===\"\n",
    "        ####MOVE HARE\n",
    "        #random action\n",
    "        nextMoveHare= rnd.randint(1, 5)\n",
    "        #print \"action\"\n",
    "        #print nextMoveHare\n",
    "        \n",
    "        #probability of moving\n",
    "        resultH= np.random.choice(a,1, replace=False, p=hareP)  #what about the .6 probability of not moving? ???        \n",
    "        #print \"probability action\"\n",
    "        #print  resultH\n",
    "       \n",
    "        if resultH!=[\"S\"]:\n",
    "            if nextMoveHare == 1 or nextMoveHare == 2:\n",
    "                afterHare = (nextPos[0], moveUpDown(nextPos[1]))\n",
    "            else:\n",
    "                afterHare = (nextPos[0], moveLeftRight(nextPos[1]))\n",
    "        else:\n",
    "            afterHare = nextPos\n",
    "        \n",
    "        #print afterHare\n",
    "        \n",
    "        \n",
    "        ##state \n",
    "        #print \"=state=\"\n",
    "        stateIdx=states.index(afterHare)\n",
    "        state= states[stateIdx]        \n",
    "        #print stateIdx\n",
    "        #print state\n",
    "         \n",
    "        #print \"===wolf===\"\n",
    "    \n",
    "        ####MOVE WOLF\n",
    "        #print policy_J_optimal[stateIdx]\n",
    "        nextMoveV = max(policy_J_optimal[stateIdx])\n",
    "        #print nextMoveV\n",
    "        \n",
    "        if nextMoveV!=1:\n",
    "            resultW= np.random.choice(a,1, replace=False, p=wolfP)  #what about the .6 probability of not moving? ???        \n",
    "            \n",
    "            #print resultW\n",
    "            if resultW!=[\"S\"]:\n",
    "            \n",
    "                if nextMoveV==0.25:\n",
    "                    nextMoveWolf= rnd.randint(1, 5)                           \n",
    "                elif policy_J_optimal[stateIdx][0]==0.5:\n",
    "                    \n",
    "                    nextMoveWolf= rnd.randint(1, 3)\n",
    "                else:\n",
    "                    nextMoveWolf= rnd.randint(3, 5)\n",
    "                \n",
    "                #print nextMoveWolf\n",
    "                if nextMoveWolf == 1 or nextMoveWolf == 2:\n",
    "                    afterWolf = (moveUpDown(state[0]), state[1])\n",
    "                \n",
    "                else:\n",
    "                    afterWolf = (moveLeftRight(state[0]), state[1])\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                afterWolf= state\n",
    "                            \n",
    "        else:\n",
    "            afterWolf= state\n",
    "        \n",
    "        \n",
    "        #print afterWolf\n",
    "        #print \"\\n\"\n",
    "        \n",
    "    \n",
    "        #nextPos = final\n",
    "        nextPos = afterWolf\n",
    "        trajectory+=[nextPos]\n",
    "        #takes into account policy and cost?\n",
    "        \n",
    "        #count discounted cost\n",
    "        s=s+1\n",
    "        \n",
    "    \n",
    "    t=t+1\n",
    "    trajectories += [trajectory]\n",
    "    \n",
    "for i in trajectories:\n",
    "    print i\n",
    "    print \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
