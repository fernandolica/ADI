{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 5: Supervised learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. The IRIS dataset\n",
    "\n",
    "The Iris flower data set is a data set describing the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gasp√© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (_Iris setosa_, _Iris virginica_ and _Iris versicolor_). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres.\n",
    "\n",
    "In your work, you will use only two of the four features and consider only two of the three species of Iris.\n",
    "\n",
    "---\n",
    "\n",
    "We start by loading the dataset and plotting the two classes of points that we wish to discriminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)\n",
    "\n",
    "X = iris.data[50:,2:]\n",
    "a = iris.target[50:]\n",
    "\n",
    "# Plot data\n",
    "#plt.plot(X[:50, 0], X[:50, 1], 'bx', label='Versicolour')\n",
    "#plt.plot(X[50:, 0], X[50:, 1], 'ro', label='Virginica')\n",
    "#plt.xlabel('Petal length (cm)')\n",
    "#plt.ylabel('Petal width (cm)')\n",
    "#plt.legend(loc='best')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Train a logistic regression classifier in Python using Newton-Raphson's method. The method is described by the update:\n",
    "\n",
    "$$\\mathbf{w}^{(k+1)}\\leftarrow\\mathbf{w}^{(k)}-\\mathbf{H}^{-1}\\mathbf{g},$$\n",
    "\n",
    "where $\\mathbf{H}$ and $\\mathbf{g}$ are the _Hessian matrix_ and _gradient vector_ that you computed in your homework. Therefore, to train the classifier you should write a cycle that repeatedly updates the parameter vector according to the rule above until the difference between two iterations is sufficiently small (e.g., smaller than $10^{-5}$).\n",
    "\n",
    "Print the resulting parameters and plot the decision boundary over the data points. Make sure that:\n",
    "\n",
    "1. You augment your data pointa with an extra coordinate that is always 1\n",
    "2. The output vector takes only values 0 and 1\n",
    "3. You initialize your parameters to zero.\n",
    "\n",
    "**Note:** Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.75453232]\n",
      " [ 10.44669989]\n",
      " [-45.27234377]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# <Insert your code here>\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def predict(y,x,w):\n",
    "    for n in range(x.shape[0]):\n",
    "        xn = x[n,:].reshape((3,1))\n",
    "        \n",
    "        if (exp(xn,w) > 1 - exp(xn,w)):\n",
    "            y[n] = 1\n",
    "        else:\n",
    "            y[n] = 0\n",
    "    return y\n",
    "\n",
    "def exp(W, Xi):\n",
    "    return 1/(1+np.exp(-((W.T).dot(Xi))))\n",
    "\n",
    "\n",
    "def gradient(y,x,w):\n",
    "    \n",
    "    result = 0\n",
    "\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        Xi = x[i,:].reshape((3,1))\n",
    "        \n",
    "        result += np.dot(Xi, (y[i] - (exp(w, Xi))))\n",
    "  \n",
    "    return result  \n",
    "                       \n",
    "def hessian(x,w):\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        Xi = x[i,:].reshape((3,1))\n",
    "        \n",
    "        aux1 = Xi.dot(Xi.T) \n",
    "        aux2 = aux1 *(exp(Xi, w)) \n",
    "        aux3 = aux2 *(1-exp(Xi, w))\n",
    " \n",
    "        result += aux3\n",
    "    \n",
    "    return -result \n",
    "                        \n",
    "\n",
    "#print \"g:\", gradient(a,x,w)\n",
    "\n",
    "#print \"h:\", hessian(x,w)\n",
    "    \n",
    "    \n",
    "def newton_raphson(y,x,w):\n",
    "    \n",
    "    #k = np.array([[0],[0],[0]])\n",
    "    \n",
    "    g = gradient(y,x,w)\n",
    "    \n",
    "    h = hessian (x,w)\n",
    "    \n",
    "    #print h\n",
    "\n",
    "    new_w = w - np.dot(np.linalg.inv(h),g)\n",
    "    \n",
    "    return new_w\n",
    "\n",
    "def iterate(y,x,w):\n",
    "\n",
    "    err = 1\n",
    "        \n",
    "    while err > 10e-5:\n",
    "        \n",
    "        updated_w = newton_raphson(y,x,w)\n",
    "\n",
    "        err = np.linalg.norm(updated_w - w)\n",
    "\n",
    "        #print err \n",
    "        \n",
    "        w = updated_w\n",
    "\n",
    "    return updated_w\n",
    "\n",
    "#print \"gradient:\"        \n",
    "#print gradient(a,x,w)\n",
    "#print \"hessian:\"\n",
    "#print hessian(x,w)\n",
    "\n",
    "w = np.zeros((3,1))\n",
    "\n",
    "x = np.c_[ X, np.ones(100) ]\n",
    "\n",
    "y = a-1\n",
    "\n",
    "###\n",
    "\n",
    "w = iterate(y,x,w)\n",
    "\n",
    "y = predict(y,x,w)\n",
    "\n",
    "print w\n",
    "\n",
    "print y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Compare your classifier from Activity 1 with the logistic regression classifier implemented `sci-kit learn`. The code block below already loads and constructs a logistic regression model. \n",
    "\n",
    "To compare you must first fit the model to the data from Activity 1 (use the method `fit`). Next, you should build a fine grid of points $(x, y)$ in feature space (try using the `numpy` function `meshgrid`) and compute the corresponding class using the classifier (use the method `predict`). You can then use the function `plt.pcolormesh` to plot the resulting regions of decision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1e+40, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 0.          0.33333333  0.66666667  1.        ]\n",
      "[ 0.   0.5  1. ]\n",
      "[[ 0.          0.33333333  0.66666667  1.        ]\n",
      " [ 0.          0.33333333  0.66666667  1.        ]\n",
      " [ 0.          0.33333333  0.66666667  1.        ]]\n",
      "[[ 0.   0.   0.   0. ]\n",
      " [ 0.5  0.5  0.5  0.5]\n",
      " [ 1.   1.   1.   1. ]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "model = LogisticRegression(solver='newton-cg', C=1e40)\n",
    "\n",
    "# <Insert your code here>\n",
    "\n",
    "print (model.fit(X,y))\n",
    "print (model.predict(X))\n",
    "\n",
    "nx, ny = (4, 3)\n",
    "x = np.linspace(0, 1, nx)\n",
    "y = np.linspace(0, 1, ny)\n",
    "print x\n",
    "print y\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "print xv\n",
    "print yv\n",
    "\n",
    "print \"done\"\n",
    "# Plot\n",
    "#\n",
    "# Uncomment the next lines to plot the boundary learned with scikit-learn.\n",
    "\n",
    "cmap_light = ListedColormap(['#AAAAFF', '#FFAAAA'])\n",
    "# plt.pcolormesh(<Insert x1 data>, <Insert x2 data>, <Insert predicted labels>, cmap=cmap_light)\n",
    "#plt.pcolormesh(updated_w, updated_w, model.predict(X), cmap=cmap_light)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. SPAM filtering\n",
    "\n",
    "You will now implement a spam filter, in which you will compare the results of different classifiers seen in class. In order to do so, you will first need to prepare the data for learning.\n",
    "\n",
    "The following block of code illustrates how you can use the `os` module to access a list of files in a given folder. In particular, if you uncompress the file `data.zip` your working folder, the instruction `os.listdir('data')` will return a list with the contents of folder `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'nonspam-test', 'nonspam-train', 'spam-test', 'spam-train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "print(os.listdir('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uncompress the data file `data.zip` to your current folder. You will find a total of four folders, named `spam-train`, `nonspam-train`, `spam-test` and `nonsmap-test`. Each folder contains a number of text files which have been pre-processed to remove stop-words, punctuation signs, and other non-informative elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "You will now select the 3,000 most frequent words appearing in the training data. You will use the number of occurrences of these words in each e-mail as the features that describe that e-mail. The code provided already goes over all the files in the folders `*-train` and builds a dictionary (actually, a `Counter`) containing all words appearing and how often they appear. \n",
    "\n",
    "Use the information in the (`Counter`) dictionary to select the 3,000 most frequent words. Before compiling the list of most common words, make sure to remove _non-words_---for which you can use the `isalpha` method of the `str` class---and _words of length 1_. To build the list of most frequent words, you may find useful the method `most_common` of the Counter class. Make sure you end up with a _sorted list_ containing the 3,000 most frequent words. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "\n",
    "files = []\n",
    "for training_dir in ['data/spam-train', 'data/nonspam-train']:\n",
    "    files += [os.path.join(training_dir, f) for f in os.listdir(training_dir)]\n",
    "\n",
    "for f in files:\n",
    "    fin = open(f, 'r')\n",
    "    words += str(fin.read()).split()        \n",
    "    fin.close()\n",
    "\n",
    "\n",
    "d = Counter(words)\n",
    "\n",
    "# <Insert your code here>\n",
    "\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "\n",
    "for word in words:\n",
    "    #print word\n",
    "    if word.isalpha() and len(word) > 1:\n",
    "        filtered_words += [word]\n",
    "        \n",
    "\n",
    "most_common_words = Counter(filtered_words).most_common(3000)\n",
    "\n",
    "#print most_common_words\n",
    "print \"done\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each of the files in the folder `spam-train` corresponds to a datapoint $(\\mathbf{x}_n,a_n)$, where $\\mathbf{x}_n$ is a vector containing the number of times that each of the most frequent words (computed in Activity 3) appears in that file, and $a_n$ is $0$. Conversely, each of the files in the folder `nonspam-train` corresponds to a datapoint $(\\mathbf{x}_n,a_n)$, where $\\mathbf{x}_n$ is again a vector containing the number of times that each of the most frequent words appears in that file, and $a_n$ is $1$. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Go over the files in the aforementioned folders and create the a matrix `X` where each row $i$ is the datapoint corresponding to file $i$, and each column $j$ contains the number of times that the word $j$ appears in each of the files. Create the corresponding vector `a` of labels, where the component $i$ is 0 or 1 depending on whether file $i$ is spam or not.\n",
    "\n",
    "** Note: ** You may want to create a function that receives the name of a folder and a list of words as arguments and returns the matrix of datapoints corresponding to the files in that folder, where each datapoint is described as a vector of features and each feature corresponds to the number of occurrences of the words in the list provided.\n",
    "\n",
    "** Note 2: ** Extracting the features corresponding to the files may take a bit, so don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# <Insert your code here>\n",
    "\n",
    "\n",
    "def create_matrix(folder,word_list):\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    matrix = []\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for training_dir in [folder]:\n",
    "        files += [os.path.join(training_dir, f) for f in os.listdir(training_dir)]\n",
    "\n",
    "\n",
    "    for f in files:  # para testar testa apenas com um ficheiro files[:1] \n",
    "        \n",
    "        if folder == 'data/spam-train' or folder == 'data/spam-test':\n",
    "            labels += [0]\n",
    "        else:\n",
    "            labels += [1]\n",
    "        \n",
    "        words = []\n",
    "        \n",
    "        fin = open(f, 'r')\n",
    "        words += str(fin.read()).split()        \n",
    "        fin.close()\n",
    "        \n",
    "        \n",
    "        filtered_words = []\n",
    "\n",
    "        for word in words:\n",
    "            #print word\n",
    "            if word.isalpha() and len(word) > 1:\n",
    "                filtered_words += [word]\n",
    "        \n",
    "        #print filtered_words\n",
    "        \n",
    "        \n",
    "        matrix_entry = []\n",
    "        \n",
    "        word_count = Counter(filtered_words)\n",
    "        \n",
    "        \n",
    "        #print word_count       \n",
    "        for tup in word_list:\n",
    "            if tup[0] in word_count:\n",
    "                #print tup[0]\n",
    "                matrix_entry += [word_count[tup[0]]]\n",
    "            else:\n",
    "                matrix_entry += [0]\n",
    "        \n",
    "        matrix += [matrix_entry]\n",
    "     \n",
    "    #print len(word_list)\n",
    "    #print labels\n",
    "    return matrix, labels\n",
    "    \n",
    "    #for i in matrix:\n",
    "    #    print len(i)\n",
    "            \n",
    "    \n",
    "\n",
    "ocurrence_matrix_spam, trainspam_label = create_matrix('data/spam-train',most_common_words)\n",
    "ocurrence_matrix_nonspam, trainnonspam_label = create_matrix('data/nonspam-train',most_common_words)\n",
    "\n",
    "trainlabels=trainspam_label\n",
    "trainlabels+=trainnonspam_label\n",
    "\n",
    "ocurrence_matrix_train=ocurrence_matrix_spam\n",
    "ocurrence_matrix_train+=ocurrence_matrix_nonspam \n",
    "\n",
    "#print ocurrence_matrix_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you have compiled your training set, you will train three different classifiers with the same dataset: a discriminant function (SVM), a discriminative model (LR) and a generative model (NB), and compare the performance of all three in terms of training time and performance on the test set. In order to do that, you must import the three classifiers and train them, much like you did with the LR classifier in Activity 2. \n",
    "\n",
    "The three classifiers have already been constructed for you.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Train the three classifiers with the data that you collected in Activity 4. Report the time that each classifier took to train.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearSVC\n",
      "0.309000015259\n",
      "\n",
      "LogisticRegression\n",
      "0.433000087738\n",
      "\n",
      "MultinomialNB\n",
      "0.394000053406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SVM model\n",
    "svm_model = LinearSVC()\n",
    "\n",
    "# Logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# <Insert your code here>\n",
    "\n",
    "print \"\\nLinearSVC\"\n",
    "t=time.time()\n",
    "svm_model.fit(ocurrence_matrix_train,trainlabels)\n",
    "print str(time.time() - t)\n",
    "#svm_model.predict(ocurrence_matrix_train)\n",
    "\n",
    "print \"\\nLogisticRegression\"\n",
    "t=time.time()\n",
    "lr_model.fit(ocurrence_matrix_train,trainlabels)\n",
    "print str(time.time() - t)\n",
    "#lr_model.predict(ocurrence_matrix_train)\n",
    "\n",
    "print \"\\nMultinomialNB\"\n",
    "t=time.time()\n",
    "nb_model.fit(ocurrence_matrix_train,trainlabels)\n",
    "print str(time.time() - t)\n",
    "#nb_model.predict(ocurrence_matrix_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, you will test the performance of the three classifiers in the test data. To that purpose, you must read the data in the `*-test` folders into a matrix of test points and the corresponding labels, and compare your predictions in this data with the actual labels. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "For the messages in the folders `*-test` compute the predictions of your classifiers. Then, use the function `confusion_matrix` (which has been imported for you) to analyze the performance of your method. Report the accuracy of each classifier (i.e., the percentage of correct answers) and comment on the advantages and disadvantages of the three methods for this task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearSVC Accuracy\n",
      "Spam : 98.4615384615%\n",
      "NonSpam : 99.2307692308%\n",
      "\n",
      "LogisticRegression Accuracy\n",
      "Spam : 98.4615384615%\n",
      "NonSpam : 99.2307692308%\n",
      "\n",
      "MultinomialNB Accuracy\n",
      "Spam : 99.2307692308%\n",
      "NonSpam : 96.9230769231%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion_matrix(y_true, y_pred)  \n",
    "'''\n",
    "result -> (2x2) [[right wrong]  \n",
    "                 wrong right]]\n",
    "                 \n",
    "        (3x3) [[right wrong wrong ]\n",
    "                [worng right worng]    \n",
    "                [worng wrong right]]\n",
    "'''\n",
    "\n",
    "# <Insert your code here>\n",
    "\n",
    "\n",
    "\n",
    "ocurrence_matrix_stest, testspamlabel = create_matrix(\"data/spam-test\", most_common_words)\n",
    "ocurrence_matrix_nstest, testnspamlabel = create_matrix(\"data/nonspam-test\", most_common_words)\n",
    "\n",
    "auxlabels = testspamlabel\n",
    "auxlabels += testnspamlabel\n",
    "\n",
    "ocurrence_matrix_test = ocurrence_matrix_stest\n",
    "ocurrence_matrix_test += ocurrence_matrix_nstest\n",
    "\n",
    "size= len(auxlabels)/2\n",
    "\n",
    "##SVM\n",
    "svm_pred = svm_model.predict(ocurrence_matrix_stest)\n",
    "svm_confm = confusion_matrix(auxlabels, svm_pred)\n",
    "svm_accuracy_spam = float(svm_confm[0][0])/size * 100\n",
    "svm_accuracy_nonspam = float(svm_confm[1][1])/size * 100\n",
    "print \"\\nLinearSVC Accuracy\"\n",
    "print \"Spam : \" + str(svm_accuracy_spam) + \"%\"\n",
    "print \"NonSpam : \" + str(svm_accuracy_nonspam) + \"%\"\n",
    "\n",
    "##LR\n",
    "lr_pred = lr_model.predict(ocurrence_matrix_stest)\n",
    "lr_confm = confusion_matrix(auxlabels, lr_pred)\n",
    "lr_accuracy_spam = float(lr_confm[0][0])/size * 100\n",
    "lr_accuracy_nonspam = float(lr_confm[1][1])/size * 100\n",
    "print \"\\nLogisticRegression Accuracy\"\n",
    "print \"Spam : \" + str(lr_accuracy_spam) + \"%\"\n",
    "print \"NonSpam : \" + str(lr_accuracy_nonspam) + \"%\"\n",
    "\n",
    "##NB\n",
    "nb_pred = nb_model.predict(ocurrence_matrix_stest)\n",
    "nb_confm = confusion_matrix(auxlabels, nb_pred)\n",
    "nb_accuracy_spam = float(nb_confm[0][0])/size * 100\n",
    "nb_accuracy_nonspam = float(nb_confm[1][1])/size * 100\n",
    "print \"\\nMultinomialNB Accuracy\"\n",
    "print \"Spam : \" + str(nb_accuracy_spam) + \"%\"\n",
    "print \"NonSpam : \" + str(nb_accuracy_nonspam) + \"%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Insert your comment here.\n",
    "\n",
    "\n",
    "Logistic regression is great in a low number of dimensions and when the predictors don't suffice to give more than a probabilistic estimate of the response. SVMs do better when there's a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.\n",
    "\n",
    "Multinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in. \n",
    "\n",
    "SVM gets the best separating hyperplane, and they're efficient in high dimensional spaces. They're similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there's noise in the data. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
